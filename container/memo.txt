
h1. Flink SQL

- 別ターミナルで送信受信を試す
./kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-stream-flink-sql-output --from-beginning
./kafka/bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic my-stream-flink-sql-input
{"id":1, "name":"taro", "gender":"M", "age":10}
{"id":2, "name":"hanako", "gender":"F", "age":20}
{"id":3, "name":"tama", "gender":"M", "age":30}
{"id":4, "name":"tamako", "gender":"F", "age":40}

- テーブル状況を確認する
docker exec -it jobmanager /bin/bash
./bin/sql-client.sh

CREATE TABLE streams_flink_input (
    id INT,
    name STRING,
    gender STRING,
    age INT
) WITH (
    'connector' = 'kafka',
    'topic' = 'my-stream-flink-sql-input',
    'properties.bootstrap.servers' = 'broker:29092',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'json'
);

CREATE TABLE streams_flink_output (
    name STRING,
    gender STRING
) WITH (
    'connector' = 'kafka',
    'topic' = 'my-stream-flink-sql-output',
    'properties.bootstrap.servers' = 'broker:29092',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'json'
);

SHOW TABLES;
DESCRIBE streams_flink_input;
DESCRIBE streams_flink_output;

SHOW JOBS;

SELECT * FROM streams_flink_input;
SELECT * FROM streams_flink_output;

////////////////////////////

h1. Flink Job

# preparing
docker exec -it broker kafka-topics --bootstrap-server broker:29092 --list

docker exec -it broker kafka-topics --bootstrap-server broker:29092 --create --topic my-stream-flink-job-input
docker exec -it broker kafka-topics --bootstrap-server broker:29092 --create --topic my-stream-flink-job-output

# run the job
docker exec -it jobmanager ./bin/flink run --python /opt/flink/uppercase_users.py

# for consumer
docker exec -it broker kafka-console-consumer --bootstrap-server broker:29092 --topic my-stream-flink-job-output

# for producer
docker exec -it broker kafka-console-producer --bootstrap-server broker:29092 --topic my-stream-flink-job-input
> tanaka,hello pyflink
> sato,kafka is fun

////////////////////////////

h1. Schema Registory

# preparing
docker exec -it broker kafka-topics --bootstrap-server broker:29092 --list
docker exec -it broker kafka-topics --bootstrap-server broker:29092 --create --topic my-stream-schema-avro

# for consumer(normal command)
docker exec -it broker kafka-console-consumer --bootstrap-server broker:29092 --topic my-stream-schema-avro
./kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-stream-schema-avro --from-beginning

# for consumer(avro)
docker exec -it schema-registry /bin/bash
kafka-avro-console-consumer --bootstrap-server broker:29092 --topic my-stream-schema-avro --from-beginning

# for producer
docker exec -it schema-registry /bin/bash

kafka-avro-console-producer \
  --bootstrap-server broker:29092 \
  --topic my-stream-schema-avro \
  --property schema.registry.url=http://localhost:8081 \
  --property value.schema='{
    "type": "record",
    "name": "User",
    "fields": [
      {"name": "id", "type": "int"},
      {"name": "name", "type": "string"},
      {
        "name": "gender",
        "type": {
          "type": "enum",
          "name": "GenderEnum",
          "symbols": ["M", "F"]
        }
      },
      {"name": "age", "type": "int"}
    ]
  }'


入力待ちになったらデータを打ち込む
・成功データ
{"id":1, "name":"taro", "gender":"M", "age":10}
{"id":2, "name":"hanako", "gender":"F", "age":20}

・エラーデータ
{"id":3, "name":"tama", "gender":"X", "age":30}

・成功データ（余分なフィールド除去）
{"id":4, "name":"tamako", "gender":"F", "age":40, "address":"Tokyo"}

* データ送信するとWeb UIにもスキーマが見えてくる
  - http://localhost:9021/clusters/LQZ4M_EyTfyMsTkoEAnbOw/management/topics/my-stream-schema-avro/schema/value

