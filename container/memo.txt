
h1. Flink Job

# preparing
docker exec -it broker kafka-topics --bootstrap-server broker:29092 --list

docker exec -it broker kafka-topics --bootstrap-server broker:29092 --create --topic my-stream-flink-job-input
docker exec -it broker kafka-topics --bootstrap-server broker:29092 --create --topic my-stream-flink-job-output

# run the job
docker exec -it jobmanager ./bin/flink run --python /opt/flink/process_kafka.py

# for consumer
docker exec -it broker kafka-console-consumer --bootstrap-server broker:29092 --topic my-stream-flink-job-output

# for producer
docker exec -it broker kafka-console-producer --bootstrap-server broker:29092 --topic my-stream-flink-job-input
> tanaka,hello pyflink
> sato,kafka is fun

////////////////////////////

h1. Flink SQL

- 別ターミナルで送信受信を試す
./kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-stream-flink-sql-output --from-beginning
./kafka/bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic my-stream-flink-sql-input
{"id":1, "name":"taro", "gender":"M", "age":10}
{"id":2, "name":"hanako", "gender":"F", "age":20}
{"id":3, "name":"tama", "gender":"M", "age":30}
{"id":4, "name":"tamako", "gender":"F", "age":40}

- テーブル状況を確認する
docker exec -it jobmanager /bin/bash
./bin/sql-client.sh

CREATE TABLE streams_flink_input (
    id INT,
    name STRING,
    gender STRING,
    age INT
) WITH (
    'connector' = 'kafka',
    'topic' = 'my-stream-flink-sql-input',
    'properties.bootstrap.servers' = 'broker:29092',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'json'
);

CREATE TABLE streams_flink_output (
    name STRING,
    gender STRING
) WITH (
    'connector' = 'kafka',
    'topic' = 'my-stream-flink-sql-output',
    'properties.bootstrap.servers' = 'broker:29092',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'json'
);

SHOW TABLES;
DESCRIBE streams_flink_input;
DESCRIBE streams_flink_output;

SHOW JOBS;

SELECT * FROM streams_flink_input;
SELECT * FROM streams_flink_output;

////////////////////////////

h1. Schema Registory（今一つ違う気がする。web uiの管理面でトピック見てもスキーマが登録されていない...）

# 準備
docker exec -it broker kafka-topics --bootstrap-server broker:29092 --list
docker exec -it broker kafka-topics --bootstrap-server broker:29092 --create --replication-factor 1 --partitions 1 --topic users-avro

# コンシューマー
docker exec -it broker kafka-console-consumer --bootstrap-server broker:29092 --topic users-avro

# プロデューサー（コンテナに入り込む）
docker-compose exec schema-registry bash

kafka-avro-console-producer \
  --bootstrap-server broker:29092 \
  --topic users-avro \
  --property schema.registry.url=http://localhost:8081 \
  --property value.schema='{"type":"record","name":"User","fields":[{"name":"name","type":"string"},{"name":"age","type":"int"}]}'

入力待ちになったらデータを打ち込む
・成功データ
{"name": "Alice", "age": 30}

・エラーデータ
{"name": "Bob", "age": "invalid"}
